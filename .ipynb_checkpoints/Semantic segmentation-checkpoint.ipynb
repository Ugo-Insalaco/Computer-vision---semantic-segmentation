{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "360655cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import cv2\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import random_split, DataLoader\n",
    "from torchvision import transforms\n",
    "from torch.optim import Adam\n",
    "from torch.nn import BCEWithLogitsLoss\n",
    "import math\n",
    "import os\n",
    "from datetime import datetime\n",
    "from time import strftime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d435f22",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Unet2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # First layer\n",
    "        self.conv1 = nn.Conv2d(3, 64, 3, padding = 1)\n",
    "        self.conv2 = nn.Conv2d(64, 16, 3, padding = 1)\n",
    "        self.batchn1 = nn.BatchNorm2d(16)\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size= (2, 2), stride = 2)\n",
    "        \n",
    "        # Second layer\n",
    "        self.conv3 = nn.Conv2d(16, 64, 3, padding = 1)\n",
    "        self.conv4 = nn.Conv2d(64, 32, 3, padding = 1)\n",
    "        self.batchn2 = nn.BatchNorm2d(32)\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size= (2, 2), stride = 2)\n",
    "        self.drop2 = nn.Dropout(0.5)\n",
    "\n",
    "        # Third layer\n",
    "        self.conv5 = nn.Conv2d(32, 32, 3, padding = 1)\n",
    "        self.conv6 = nn.Conv2d(32, 64, 3, padding = 1)\n",
    "        self.batchn3 = nn.BatchNorm2d(64)\n",
    "        self.pool3 = nn.MaxPool2d(kernel_size= (2, 2), stride = 2)\n",
    "\n",
    "        # Fourth layer\n",
    "        self.conv7 = nn.Conv2d(64, 256, 3, padding = 1)\n",
    "        self.conv8 = nn.Conv2d(256, 128, 3, padding = 1)\n",
    "        self.batchn4 = nn.BatchNorm2d(128)\n",
    "        self.pool4 = nn.MaxPool2d(kernel_size= (2, 2), stride = 2)\n",
    "        self.drop4 = nn.Dropout(0.5)\n",
    "\n",
    "        # Fifth layer\n",
    "        self.conv9 = nn.Conv2d(128, 512, 3, padding = 1)\n",
    "        self.conv10 = nn.Conv2d(512, 256, 3, padding = 1)\n",
    "        self.batchn5 = nn.BatchNorm2d(256)\n",
    "        # Upsamling 1\n",
    "        self.upsample1 = nn.ConvTranspose2d(256, 128, kernel_size=(2,2), stride=2)\n",
    "\n",
    "        # Sixth layer\n",
    "        self.conv11 = nn.Conv2d(256, 512, 3, padding = 1)\n",
    "        self.conv12 = nn.Conv2d(512, 128, 3, padding = 1)\n",
    "        self.batchn6 = nn.BatchNorm2d(128)\n",
    "        # Upsamling 2\n",
    "        self.upsample2 = nn.ConvTranspose2d(128, 64, kernel_size=(2,2), stride=2)\n",
    "\n",
    "        # Seventh layer\n",
    "        self.conv13 = nn.Conv2d(128, 256, 3, padding = 1)\n",
    "        self.conv14 = nn.Conv2d(256, 64, 3, padding = 1)\n",
    "        self.batchn7 = nn.BatchNorm2d(64)\n",
    "        # Upsamling 3\n",
    "        self.upsample3 = nn.ConvTranspose2d(64, 32, kernel_size=(2,2), stride=2)\n",
    "\n",
    "        # Eighth layer\n",
    "        self.conv15 = nn.Conv2d(64, 64, 3, padding = 1)\n",
    "        self.conv16 = nn.Conv2d(64, 32, 3, padding = 1)\n",
    "        self.batchn8 = nn.BatchNorm2d(32)\n",
    "        # Upsamling 3\n",
    "        self.upsample4 = nn.ConvTranspose2d(32, 16, kernel_size=(2,2), stride=2)\n",
    "\n",
    "        # Ninth layer\n",
    "        self.conv17 = nn.Conv2d(32, 64, 3, padding = 1)\n",
    "        self.conv18 = nn.Conv2d(64, 16, 3, padding = 1)\n",
    "        self.batchn9 = nn.BatchNorm2d(16)\n",
    "\n",
    "        # Last layer\n",
    "        self.conv19 = nn.Conv2d(16, 1, 1, padding=0)\n",
    "\n",
    "        # We choose the activation function ReLU\n",
    "        self.activation = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # First layer\n",
    "        c1 = self.activation(self.batchn1(self.conv2(self.conv1(x))))\n",
    "        p1 = self.pool1(c1)\n",
    "#         print('c1', c1.shape)\n",
    "#         print('p1', p1.shape)\n",
    "        \n",
    "        # Second layer\n",
    "        c2 = self.activation(self.batchn2(self.conv4(self.conv3(p1))))\n",
    "        p2 = self.drop2(self.pool2(c2))\n",
    "#         print('c2', c2.shape)\n",
    "#         print('p2', p2.shape)\n",
    "\n",
    "        # Third layer\n",
    "        c3 = self.activation(self.batchn3(self.conv6(self.conv5(p2))))\n",
    "        p3 = self.pool3(c3) \n",
    "#         print('c3', c3.shape)\n",
    "#         print('p3', p3.shape) \n",
    "\n",
    "        # Fourth layer\n",
    "        c4 = self.activation(self.batchn4(self.conv8(self.conv7(p3))))\n",
    "        p4 = self.drop4(self.pool3(c4))\n",
    "#         print('c4', c4.shape)\n",
    "#         print('p4', p4.shape)\n",
    "\n",
    "        # Fifth layer\n",
    "        c5 = self.activation(self.batchn5(self.conv10(self.conv9(p4))))\n",
    "#         print('c5', c5.shape)\n",
    "        # Upsampling\n",
    "        u6 = self.upsample1(c5)\n",
    "#         print('u6', u6.shape)\n",
    "        # Adding Skip Connection \n",
    "        u6 = torch.cat([u6, c4], dim = 1)\n",
    "        \n",
    "        # Sixth layer\n",
    "        c6 = self.activation(self.batchn6(self.conv12(self.conv11(u6))))\n",
    "#         print('c6', c6.shape)\n",
    "        # Upsampling\n",
    "        u7 = self.upsample2(c6)\n",
    "        # Adding Skip Connection \n",
    "#         print('u7', u7.shape)\n",
    "        u7 = torch.cat([u7, c3], dim = 1)\n",
    "\n",
    "        # Seventh layer\n",
    "        c7 = self.activation(self.batchn7(self.conv14(self.conv13(u7))))\n",
    "        # Upsampling\n",
    "        u8 = self.upsample3(c7)\n",
    "        # Adding Skip Connection \n",
    "        u8 = torch.cat([u8, c2], dim = 1)\n",
    "\n",
    "        # Eighth layer\n",
    "        c8 = self.activation(self.batchn8(self.conv16(self.conv15(u8))))\n",
    "        # Upsampling\n",
    "        u9 = self.upsample4(c8)\n",
    "        # Adding Skip Connection \n",
    "        u9 = torch.cat([u9, c1], dim = 1)\n",
    "\n",
    "        # Ninth layer\n",
    "        c9 = self.activation(self.batchn9(self.conv18(self.conv17(u9))))\n",
    "\n",
    "        # Final output because we need values between 0 and 1\n",
    "        output = torch.sigmoid((self.conv19(c9)))\n",
    "\n",
    "        return output\n",
    "     def crop(self, encFeatures, x):\n",
    "        (_, _, H, W) = x.shape\n",
    "        encFeatures = CenterCrop([H, W])(encFeatures)\n",
    "        return encFeatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "9820e3ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Unet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Unet, self).__init__()\n",
    "        downChannels = (3, 16, 16, 32, 32, 64, 64, 128, 128, 256, 256)\n",
    "        upChannels = (256, 128, 128, 64, 64, 32, 32, 16, 16)\n",
    "        upSamplesChannels = (256, 128, 64, 32, 16)\n",
    "        finalChannel = 1\n",
    "\n",
    "        self.downConvs = []\n",
    "        for i in range(len(downChannels) - 1):\n",
    "            self.downConvs.append(nn.Conv2d(downChannels[i], downChannels[i+1], (3,3), padding = 1))\n",
    "        self.maxPool = nn.MaxPool2d(kernel_size=(2,2), stride=2)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "        self.upConvs = []\n",
    "        for i in range(len(upChannels) - 1):\n",
    "            self.upConvs.append(nn.Conv2d(upChannels[i], upChannels[i+1], (3,3), padding = 1))\n",
    "\n",
    "        self.upSamples = []\n",
    "        for i in range(len(upSamplesChannels) -1):\n",
    "            self.upSamples.append(nn.ConvTranspose2d(upSamplesChannels[i], upSamplesChannels[i+1], kernel_size=(2,2), stride=2))\n",
    "\n",
    "        self.finalConv = nn.Conv2d(upChannels[-1], finalChannel, (3,3), padding = 1)\n",
    "        \n",
    "    def forward(self, input):\n",
    "        output = self.downConvs[0](input)\n",
    "        output = self.downConvs[1](output)\n",
    "        intermediates = []\n",
    "        for i in range(2, len(self.downConvs)):\n",
    "            if(i%2==0):\n",
    "                intermediates.append(output)\n",
    "                output = self.maxPool(output)\n",
    "            output = self.downConvs[i](output)\n",
    "            print(output.shape)\n",
    "        \n",
    "        for i in range(len(self.upSamples)):\n",
    "            output = self.upSamples[i](output)\n",
    "            intermed = self.crop(intermediates[-(i+1)], output)\n",
    "            output = torch.cat([output, intermed], dim=1)\n",
    "            output = self.upConvs[2*i](output)\n",
    "            output = self.upConvs[2*i+1](output)\n",
    "            print(output.shape)\n",
    "\n",
    "        output = self.finalConv(output)\n",
    "        return output\n",
    "    \n",
    "    def crop(self, encFeatures, x):\n",
    "        (_, _, H, W) = x.shape\n",
    "        encFeatures = CenterCrop([H, W])(encFeatures)\n",
    "        return encFeatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "531c84e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SegmentationDataset(Dataset):\n",
    "    def __init__(self, imagePaths, maskPaths, transforms):\n",
    "        # store the image and mask filepaths, and augmentation\n",
    "        # transforms\n",
    "        self.imagePaths = imagePaths\n",
    "        self.maskPaths = maskPaths\n",
    "        self.transforms = transforms\n",
    "    def __len__(self):\n",
    "        # return the number of total samples contained in the dataset\n",
    "        return len(self.imagePaths)\n",
    "    def __getitem__(self, idx):\n",
    "        # grab the image path from the current index\n",
    "        imagePath = self.imagePaths[idx]\n",
    "        # load the image from disk, swap its channels from BGR to RGB,\n",
    "        # and read the associated mask from disk in grayscale mode\n",
    "        image = cv2.imread(imagePath)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        mask = cv2.imread(self.maskPaths[idx], 0)\n",
    "        # check to see if we are applying any transformations\n",
    "        if self.transforms is not None:\n",
    "            # apply the transformations to both image and its mask\n",
    "            image = self.transforms(image)\n",
    "            mask = self.transforms(mask)\n",
    "        # return a tuple of the image and its mask\n",
    "        return (image, mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d80a43f4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ac537d5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():  \n",
    "    dev = \"cuda:0\" \n",
    "else:  \n",
    "    dev = \"cpu\" \n",
    "\n",
    "device = torch.device(dev)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "7bb53eb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_val_split(imagePath, maskPath):\n",
    "    # Loading images\n",
    "    baseImages = os.listdir(imagePath)\n",
    "    baseMasks = os.listdir(maskPath)\n",
    "    images = [os.path.join(imagePath, image) for image in baseImages]\n",
    "    masks = [os.path.join(imagePath, mask) for mask in baseMasks]\n",
    "    print(len(images))\n",
    "    \n",
    "    split = [0.8, 0.1, 0.1]\n",
    "    splitLength = [math.floor(fraction*len(images)) for fraction in split]\n",
    "    split = random_split(range(len(images)),splitLength, generator=torch.Generator().manual_seed(8))\n",
    "    trainImages = [images[i] for i in split[0]]\n",
    "    trainMasks = [images[i] for i in split[0]]\n",
    "    testImages = [images[i] for i in split[1]]\n",
    "    testMasks = [images[i] for i in split[1]]\n",
    "    valImages = [images[i] for i in split[2]]\n",
    "    valMasks = [images[i] for i in split[2]]\n",
    "\n",
    "    trainSize = len(trainImages)\n",
    "    testSize = len(testImages)\n",
    "    valSize = len(valImages)\n",
    "\n",
    "    transform = transforms.Compose([transforms.ToTensor(), transforms.Resize((128, 128))])\n",
    "    trainDs = SegmentationDataset(trainImages, trainMasks, transform)\n",
    "    testDs = SegmentationDataset(testImages, testMasks, transform)\n",
    "    valDs = SegmentationDataset(valImages, valMasks, transform)\n",
    "    \n",
    "    return trainDs, testDs, valDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "136ca234",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4000\n"
     ]
    }
   ],
   "source": [
    "imagePath = \"./data/images\"\n",
    "maskPath = \"./data/masks\"\n",
    "trainDs, testDs, valDs = train_test_val_split(imagePath, maskPath)\n",
    "trainSize = len(trainDs)\n",
    "testSize = len(testDs)\n",
    "valSize = len(valDs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "16db029b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model):\n",
    "    batchSize = 32\n",
    "    lr = 0.005\n",
    "    epochs = 40\n",
    "    \n",
    "    path = \"./models\"\n",
    "    time = datetime.now().strftime(\"%H-%M-%S\")\n",
    "    folder = os.path.join(path, time)\n",
    "    os.mkdir(folder)\n",
    "    checkpoint = 5\n",
    "    \n",
    "    trainLoader = DataLoader(trainDs, shuffle=True, batch_size=batchSize)\n",
    "    testLoader = DataLoader(testDs, shuffle=True, batch_size=batchSize)\n",
    "    valLoader = DataLoader(valDs, shuffle=True, batch_size=batchSize)\n",
    "\n",
    "    lossFunc = BCEWithLogitsLoss()\n",
    "    opt = Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    trainLosses = []\n",
    "    valLosses = []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        print(f\"epoch: {epoch}/{epochs}\")\n",
    "        model.train()\n",
    "        trainLoss = 0\n",
    "        valLoss = 0\n",
    "        trainSteps = 0\n",
    "        testSteps = 0\n",
    "        for (x,y) in trainLoader:\n",
    "            print(f\"batch: {trainSteps}/{math.floor(trainSize/batchSize)}\")\n",
    "            trainSteps +=1\n",
    "            pred = model(x)\n",
    "#             yc = model.crop(y, pred)\n",
    "            loss = lossFunc(pred, yc)\n",
    "\n",
    "            opt.zero_grad()\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "\n",
    "            trainLoss+=loss\n",
    "            \n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "            for (x, y) in valLoader:\n",
    "                testSteps+=1\n",
    "                pred = model(x)\n",
    "                yc = model.crop(y, pred)\n",
    "                valLoss += lossFunc(pred, yc)\n",
    "\n",
    "        trainLosses.append(trainLoss.item()/trainSteps)\n",
    "        valLosses.append(valLoss.item()/testSteps)\n",
    "        \n",
    "        if(epoch%checkpoint == 0):\n",
    "            torch.save(model, os.path.join(folder, f'epoch-{epoch}.pt'))\n",
    "        return trainLosses, valLosses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "a5704fc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0/40\n",
      "batch: 0/100\n",
      "c1 torch.Size([32, 16, 128, 128])\n",
      "p1 torch.Size([32, 16, 64, 64])\n",
      "c2 torch.Size([32, 32, 64, 64])\n",
      "p2 torch.Size([32, 32, 32, 32])\n",
      "c3 torch.Size([32, 64, 32, 32])\n",
      "p3 torch.Size([32, 64, 16, 16])\n",
      "c4 torch.Size([32, 128, 16, 16])\n",
      "p4 torch.Size([32, 128, 8, 8])\n",
      "c5 torch.Size([32, 256, 8, 8])\n",
      "u6 torch.Size([32, 128, 16, 16])\n",
      "c6 torch.Size([32, 128, 16, 16])\n",
      "u7 torch.Size([32, 64, 32, 32])\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Unet2' object has no attribute 'crop'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_15832\\248786609.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mUnet2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_15832\\989843396.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(model)\u001b[0m\n\u001b[0;32m     31\u001b[0m             \u001b[0mtrainSteps\u001b[0m \u001b[1;33m+=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m             \u001b[0mpred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 33\u001b[1;33m             \u001b[0myc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     34\u001b[0m             \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlossFunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0myc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   1205\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mmodules\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1206\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mmodules\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1207\u001b[1;33m         raise AttributeError(\"'{}' object has no attribute '{}'\".format(\n\u001b[0m\u001b[0;32m   1208\u001b[0m             type(self).__name__, name))\n\u001b[0;32m   1209\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Unet2' object has no attribute 'crop'"
     ]
    }
   ],
   "source": [
    "model = Unet2()\n",
    "train(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "5d511a1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'15-25-10'"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datetime.now().strftime(\"%H-%M-%S\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f2d4c87",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
